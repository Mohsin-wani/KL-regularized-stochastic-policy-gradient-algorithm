{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pybullet_envs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=1888):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <HalfCheetahBulletEnv<HalfCheetahBulletEnv-v0>>\n",
      "State shape: (26,)\n",
      "Action shape: (6,)\n",
      "action space Box(-1.0, 1.0, (6,), float32) observation space : Box(-inf, inf, (26,), float32)\n",
      "action space bound :[-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.]\n",
      "action limit = 1.0 dimension 6\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = 'HalfCheetahBulletEnv-v0' #  'HopperBulletEnv-v0' # 'AntBulletEnv-v0' \n",
    "seed = 1888\n",
    "env = make_env(env_name,seed )\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_p(tf.keras.Model):\n",
    "    def __init__(self, act_dim, act_limit):\n",
    "        super(Policy_p, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(400, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(300, activation=\"relu\")\n",
    "        self.mean = tf.keras.layers.Dense(act_dim, activation = 'tanh')\n",
    "        self.log_std_dev = tf.keras.layers.Dense(act_dim, activation='linear')\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        mu = self.mean(x)\n",
    "        log_std = self.log_std_dev(x)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "\n",
    "        mu, log_std = self.call(state)\n",
    "        std_dev = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mu,std_dev)\n",
    "        z = normal.sample() \n",
    "        actions = tf.clip_by_value(z,-self.act_limit,self.act_limit)\n",
    "        probs = normal.prob(actions)\n",
    "        \n",
    "        return actions, probs, mu, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, mu, log_std,  reward, next_state, done):\n",
    "        item = (state, action, mu, log_std, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions,  mus, log_stds, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(mus, np.float32), np.array(log_stds, np.float32),  np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, mus,log_stds, rewards, next_states, done_flags = self.sample(32)\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        mus = np.reshape(mus, (-1,act_dim))\n",
    "        log_stds = np.reshape(log_stds, (-1,act_dim))\n",
    "\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        mus_ts = tf.convert_to_tensor(mus, dtype = tf.float32)\n",
    "        log_stds_ts = tf.convert_to_tensor(log_stds, dtype = tf.float32)\n",
    "\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        return state_ts, action_ts, mus_ts, log_stds_ts, reward_ts, next_state_ts, done_flags\n",
    "    \n",
    "    def initialize_replay_buffer(self,env, n_steps = 100):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            buffer.add(state, action,action,action, reward, next_state, done) #action values added as place holders during initialization\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3 with KL regularizer\n",
    "class AgentTD3E:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.dist = tfp.distributions.Normal(0,0.3)\n",
    "        self.learning_rate_critic = 0.002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic()\n",
    "        self.critic2 = Critic()\n",
    "\n",
    "        self.target_critic1 = Critic()\n",
    "        self.target_critic2 = Critic()\n",
    "\n",
    "        self.policy = Policy_p(act_dim,act_limit)\n",
    "        self.target_policy = Policy_p(act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _,_ = self.policy(s)\n",
    "        _,_ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99, eps=1e-6):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions, _, _, _ = self.target_policy.act(next_states)\n",
    "          \n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    #@tf.function\n",
    "    def compute_p_loss(self,states, mean_prev, log_std_prev, eps=1e-6):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions, _, mu_s, log_stds = self.policy.act(states) \n",
    "\n",
    "\n",
    "            kl = (log_stds - log_std_prev) + ( tf.square(tf.exp(log_std_prev)) + ((mu_s - mean_prev) **2) ) / (2 * tf.square(tf.exp(log_stds))) - 0.5 \n",
    "            kl = tf.reduce_mean(tf.reduce_sum(kl, axis = -1))        \n",
    "            policy_loss = - (self.critic1(states,actions) - (0.01 * kl))\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss, kl\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, mus, log_stds, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "\n",
    "        p_loss, kl = self.compute_p_loss(states, mus, log_stds)\n",
    "                \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2, kl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ---------\n",
      "absl-py                      1.4.0\n",
      "aiohttp                      3.8.4\n",
      "aiosignal                    1.3.1\n",
      "ale-py                       0.7.5\n",
      "appdirs                      1.4.4\n",
      "argcomplete                  3.0.8\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "async-timeout                4.0.2\n",
      "atari-py                     1.2.2\n",
      "atomicwrites                 1.4.1\n",
      "attrs                        23.1.0\n",
      "AutoROM                      0.4.2\n",
      "AutoROM.accept-rom-license   0.6.1\n",
      "backcall                     0.2.0\n",
      "boto                         2.49.0\n",
      "Box2D                        2.3.2\n",
      "box2d-py                     2.3.5\n",
      "cachetools                   5.3.0\n",
      "certifi                      2022.12.7\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           3.1.0\n",
      "click                        8.1.3\n",
      "cloudpickle                  2.2.1\n",
      "colorama                     0.4.6\n",
      "comm                         0.1.3\n",
      "contourpy                    1.0.7\n",
      "crcmod                       1.7\n",
      "cryptography                 40.0.2\n",
      "cycler                       0.11.0\n",
      "Cython                       0.29.34\n",
      "d4rl-atari                   0.1\n",
      "d4rl-pybullet                0.1\n",
      "debugpy                      1.6.7\n",
      "decorator                    5.1.1\n",
      "dm-tree                      0.1.8\n",
      "docker-pycreds               0.4.0\n",
      "executing                    1.2.0\n",
      "fasteners                    0.18\n",
      "flatbuffers                  1.12\n",
      "fonttools                    4.39.3\n",
      "frozenlist                   1.3.3\n",
      "gast                         0.4.0\n",
      "gcs-oauth2-boto-plugin       3.0\n",
      "gitdb                        4.0.10\n",
      "GitPython                    3.1.31\n",
      "glfw                         2.5.9\n",
      "google-apitools              0.5.32\n",
      "google-auth                  2.17.3\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "google-reauth                0.1.1\n",
      "grpcio                       1.54.0\n",
      "gsutil                       5.23\n",
      "gym                          0.25.1\n",
      "gym-notices                  0.0.8\n",
      "gym-wrappers                 0.1.0\n",
      "h5py                         3.8.0\n",
      "httplib2                     0.20.4\n",
      "idna                         3.4\n",
      "imageio                      2.27.0\n",
      "imageio-ffmpeg               0.4.8\n",
      "importlib-metadata           6.5.0\n",
      "importlib-resources          5.12.0\n",
      "iniconfig                    2.0.0\n",
      "ipykernel                    6.20.1\n",
      "ipython                      8.12.0\n",
      "jedi                         0.18.2\n",
      "Jinja2                       3.1.3\n",
      "joblib                       1.2.0\n",
      "jupyter_client               8.2.0\n",
      "jupyter_core                 5.3.0\n",
      "keras                        2.9.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.4\n",
      "libclang                     16.0.0\n",
      "llvmlite                     0.43.0\n",
      "lz4                          4.3.2\n",
      "Markdown                     3.4.3\n",
      "MarkupSafe                   2.1.2\n",
      "matplotlib                   3.7.1\n",
      "matplotlib-inline            0.1.6\n",
      "monotonic                    1.6\n",
      "mujoco                       2.2.0\n",
      "mujoco-py                    2.1.2.14\n",
      "multidict                    6.0.4\n",
      "nest-asyncio                 1.5.6\n",
      "numba                        0.60.0\n",
      "numpy                        1.24.2\n",
      "oauth2client                 4.1.3\n",
      "oauthlib                     3.2.2\n",
      "opencv-python                4.7.0.72\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    23.1\n",
      "pandas                       2.0.0\n",
      "parso                        0.8.3\n",
      "pathtools                    0.1.2\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       9.5.0\n",
      "pip                          23.0.1\n",
      "platformdirs                 3.2.0\n",
      "pluggy                       1.0.0\n",
      "prompt-toolkit               3.0.38\n",
      "protobuf                     3.19.6\n",
      "psutil                       5.9.5\n",
      "pure-eval                    0.2.2\n",
      "py                           1.11.0\n",
      "pyasn1                       0.4.8\n",
      "pyasn1-modules               0.2.8\n",
      "pybullet                     3.2.5\n",
      "pycparser                    2.21\n",
      "pygame                       2.1.0\n",
      "pyglet                       2.0.0\n",
      "Pygments                     2.15.1\n",
      "PyOpenGL                     3.1.6\n",
      "pyOpenSSL                    23.1.1\n",
      "pyparsing                    3.0.9\n",
      "pytest                       7.0.1\n",
      "python-dateutil              2.8.2\n",
      "pytz                         2023.3\n",
      "pyu2f                        0.1.5\n",
      "pywin32                      306\n",
      "PyYAML                       6.0\n",
      "pyzmq                        25.0.2\n",
      "requests                     2.28.2\n",
      "requests-oauthlib            1.3.1\n",
      "retry-decorator              1.1.1\n",
      "rsa                          4.7.2\n",
      "scikit-learn                 1.2.2\n",
      "scipy                        1.10.1\n",
      "sentry-sdk                   1.24.0\n",
      "setproctitle                 1.3.2\n",
      "setuptools                   66.0.0\n",
      "six                          1.16.0\n",
      "smmap                        5.0.0\n",
      "stack-data                   0.6.2\n",
      "tensorboard                  2.9.1\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.9.0\n",
      "tensorflow-estimator         2.9.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "tensorflow-probability       0.16.0\n",
      "termcolor                    2.2.0\n",
      "threadpoolctl                3.1.0\n",
      "tomli                        2.0.1\n",
      "tornado                      6.3\n",
      "tqdm                         4.65.0\n",
      "traitlets                    5.9.0\n",
      "typing_extensions            4.5.0\n",
      "tzdata                       2023.3\n",
      "urllib3                      1.26.15\n",
      "wandb                        0.15.3\n",
      "wcwidth                      0.2.6\n",
      "Werkzeug                     2.2.3\n",
      "wheel                        0.38.4\n",
      "wrapt                        1.15.0\n",
      "yarl                         1.9.2\n",
      "zipp                         3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gymv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
