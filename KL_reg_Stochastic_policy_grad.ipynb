{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pybullet_envs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=1888):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <HalfCheetahBulletEnv<HalfCheetahBulletEnv-v0>>\n",
      "State shape: (26,)\n",
      "Action shape: (6,)\n",
      "action space Box(-1.0, 1.0, (6,), float32) observation space : Box(-inf, inf, (26,), float32)\n",
      "action space bound :[-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.]\n",
      "action limit = 1.0 dimension 6\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = 'HalfCheetahBulletEnv-v0' #  'HopperBulletEnv-v0' # 'AntBulletEnv-v0' \n",
    "seed = 1888\n",
    "env = make_env(env_name,seed )\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_p(tf.keras.Model):\n",
    "    def __init__(self, act_dim, act_limit):\n",
    "        super(Policy_p, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(400, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(300, activation=\"relu\")\n",
    "        self.mean = tf.keras.layers.Dense(act_dim, activation = 'tanh')\n",
    "        self.log_std_dev = tf.keras.layers.Dense(act_dim, activation='linear')\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        mu = self.mean(x)\n",
    "        log_std = self.log_std_dev(x)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "\n",
    "        mu, log_std = self.call(state)\n",
    "        std_dev = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mu,std_dev)\n",
    "        z = normal.sample() \n",
    "        actions = tf.clip_by_value(z,-self.act_limit,self.act_limit)\n",
    "        probs = normal.prob(actions)\n",
    "        \n",
    "        return actions, probs, mu, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, mu, log_std,  reward, next_state, done):\n",
    "        item = (state, action, mu, log_std, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions,  mus, log_stds, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(mus, np.float32), np.array(log_stds, np.float32),  np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, mus,log_stds, rewards, next_states, done_flags = self.sample(32)\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        mus = np.reshape(mus, (-1,act_dim))\n",
    "        log_stds = np.reshape(log_stds, (-1,act_dim))\n",
    "\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        mus_ts = tf.convert_to_tensor(mus, dtype = tf.float32)\n",
    "        log_stds_ts = tf.convert_to_tensor(log_stds, dtype = tf.float32)\n",
    "\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        return state_ts, action_ts, mus_ts, log_stds_ts, reward_ts, next_state_ts, done_flags\n",
    "    \n",
    "    def initialize_replay_buffer(self,env, n_steps = 100):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            buffer.add(state, action,action,action, reward, next_state, done) #action values added as place holders during initialization\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3 with KL regularizer\n",
    "class AgentTD3E:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.dist = tfp.distributions.Normal(0,0.3)\n",
    "        self.learning_rate_critic = 0.002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic()\n",
    "        self.critic2 = Critic()\n",
    "\n",
    "        self.target_critic1 = Critic()\n",
    "        self.target_critic2 = Critic()\n",
    "\n",
    "        self.policy = Policy_p(act_dim,act_limit)\n",
    "        self.target_policy = Policy_p(act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _,_ = self.policy(s)\n",
    "        _,_ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99, eps=1e-6):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions, _, _, _ = self.target_policy.act(next_states)\n",
    "          \n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    #@tf.function\n",
    "    def compute_p_loss(self,states, mean_prev, log_std_prev, eps=1e-6):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions, _, mu_s, log_stds = self.policy.act(states) \n",
    "\n",
    "\n",
    "            kl = (log_stds - log_std_prev) + ( tf.square(tf.exp(log_std_prev)) + ((mu_s - mean_prev) **2) ) / (2 * tf.square(tf.exp(log_stds))) - 0.5 \n",
    "            kl = tf.reduce_mean(tf.reduce_sum(kl, axis = -1))        \n",
    "            policy_loss = - (self.critic1(states,actions) - (0.01 * kl))\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss, kl\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, mus, log_stds, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "\n",
    "        p_loss, kl = self.compute_p_loss(states, mus, log_stds)\n",
    "                \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2, kl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gymv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
