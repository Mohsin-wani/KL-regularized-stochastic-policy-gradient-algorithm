{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pybullet_envs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=1888):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <HalfCheetahBulletEnv<HalfCheetahBulletEnv-v0>>\n",
      "State shape: (26,)\n",
      "Action shape: (6,)\n",
      "action space Box(-1.0, 1.0, (6,), float32) observation space : Box(-inf, inf, (26,), float32)\n",
      "action space bound :[-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.]\n",
      "action limit = 1.0 dimension 6\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\env_gymv3\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\env_gymv3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = 'HalfCheetahBulletEnv-v0' #  'HopperBulletEnv-v0' # 'AntBulletEnv-v0' \n",
    "seed = 1888\n",
    "env = make_env(env_name,seed )\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "#env = env.reset()\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_p(tf.keras.Model):\n",
    "    def __init__(self, act_dim, act_limit):\n",
    "        super(Policy_p, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(400, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(300, activation=\"relu\")\n",
    "        #self.fcb2 = tf.keras.layers.BatchNormalization()\n",
    "        self.mean = tf.keras.layers.Dense(act_dim, activation = 'tanh')\n",
    "        self.log_std_dev = tf.keras.layers.Dense(act_dim, activation='linear')\n",
    "        #self.actor = tf.keras.layers.Dense(act_dim)\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        mu = self.mean(x)\n",
    "        log_std = self.log_std_dev(x)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "        #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "\n",
    "        mu, log_std = self.call(state)\n",
    "        std_dev = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mu,std_dev)\n",
    "        z = normal.sample() \n",
    "        actions = tf.clip_by_value(z,-self.act_limit,self.act_limit)\n",
    "        probs = normal.prob(actions)\n",
    "        #log_probs = normal.log_prob(actions)\n",
    "        \n",
    "        return actions, probs, mu, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, mu, log_std,  reward, next_state, done):\n",
    "        item = (state, action, mu, log_std, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions,  mus, log_stds, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(mus, np.float32), np.array(log_stds, np.float32),  np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, mus,log_stds, rewards, next_states, done_flags = self.sample(32)\n",
    "        #print(type(states))\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        mus = np.reshape(mus, (-1,act_dim))\n",
    "        log_stds = np.reshape(log_stds, (-1,act_dim))\n",
    "\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        mus_ts = tf.convert_to_tensor(mus, dtype = tf.float32)\n",
    "        log_stds_ts = tf.convert_to_tensor(log_stds, dtype = tf.float32)\n",
    "\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        #print(f'Tensor states {state_ts} actions : {action_ts} log p {log_probs_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags}')\n",
    "        ##print(f'log_probs : {log_probs_ts} mus : {mus_ts} log_stds_ts {log_stds_ts}')\n",
    "        return state_ts, action_ts, mus_ts, log_stds_ts, reward_ts, next_state_ts, done_flags\n",
    "    \n",
    "    def initialize_replay_buffer(self,env, n_steps = 100):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} reward {reward} next state : {next_state} done : {done}')\n",
    "            buffer.add(state, action,action,action, reward, next_state, done) #action values added as place holders during initialization\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3 with KL regularizer\n",
    "class AgentTD3E:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.dist = tfp.distributions.Normal(0,0.3)\n",
    "        self.learning_rate_critic = 0.002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic()\n",
    "        self.critic2 = Critic()\n",
    "\n",
    "        self.target_critic1 = Critic()\n",
    "        self.target_critic2 = Critic()\n",
    "\n",
    "        self.policy = Policy_p(act_dim,act_limit)\n",
    "        self.target_policy = Policy_p(act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _,_ = self.policy(s)\n",
    "        _,_ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99, eps=1e-6):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions, _, _, _ = self.target_policy.act(next_states) #self.target_policy.act(next_states)\n",
    "            #noise = tf.clip_by_value(tf.random.normal(shape = target_actions.shape, mean=0, stddev=0.5),-0.5,0.5)\n",
    "            #target_actions += noise\n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "\n",
    "            #critic_loss1 = tf.keras.losses.MSE(target_qval, qval1)\n",
    "            #critic_loss2 = tf.keras.losses.MSE(target_qval, qval2)\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "            #critic_loss1 += 0.1*rev_kl\n",
    "            #critic_loss2 += 0.1*rev_kl \n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        #print(f'target actions : {target_actions} states : {states} target qv1 : {target_qval1} target qv2 : {target_qval2} qval 1 : {qval1} qval2: {qval2} target_qval : {target_qval} critic loss1 : {critic_loss1} critic loss : {critic_loss2} noise: {noise}')\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    #@tf.function\n",
    "    def compute_p_loss(self,states, mean_prev, log_std_prev, eps=1e-6):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions, _, mu_s, log_stds = self.policy.act(states) \n",
    "\n",
    "\n",
    "            kl = (log_stds - log_std_prev) + ( tf.square(tf.exp(log_std_prev)) + ((mu_s - mean_prev) **2) ) / (2 * tf.square(tf.exp(log_stds))) - 0.5 \n",
    "            kl = tf.reduce_mean(tf.reduce_sum(kl, axis = -1))        \n",
    "            policy_loss = - (self.critic1(states,actions) - (0.01 * kl))\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss, kl\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, mus, log_stds, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "\n",
    "        p_loss, kl = self.compute_p_loss(states, mus, log_stds)\n",
    "                \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2, kl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 9 avg reward : -3.1832132772416513 policy loss : -42069.0703125 KL divergence : 19493.40234375\n",
      "after 19 avg reward : 553.555886991647 policy loss : -57811.8828125 KL divergence : 13878.279296875\n",
      "after 29 avg reward : 753.2247967071582 policy loss : -64638.17578125 KL divergence : 11660.0087890625\n",
      "after 39 avg reward : 843.0152480559124 policy loss : -76145.0234375 KL divergence : 11641.5625\n",
      "after 49 avg reward : 944.8065812958317 policy loss : -83781.6171875 KL divergence : 11083.189453125\n",
      "after 59 avg reward : 1058.5326984158378 policy loss : -88661.34375 KL divergence : 11067.376953125\n",
      "after 69 avg reward : 1112.3973650512958 policy loss : -95417.203125 KL divergence : 11691.1357421875\n",
      "after 79 avg reward : 1244.4843048919915 policy loss : -102215.5234375 KL divergence : 12289.845703125\n",
      "after 89 avg reward : 1282.4324106728566 policy loss : -109179.2890625 KL divergence : 12310.15234375\n",
      "after 99 avg reward : 1250.0450945594293 policy loss : -112580.8984375 KL divergence : 12508.1044921875\n",
      "after 109 avg reward : 1332.1102814770861 policy loss : -118835.890625 KL divergence : 11567.6826171875\n",
      "after 119 avg reward : 1373.8077592929765 policy loss : -122630.6484375 KL divergence : 10561.349609375\n",
      "after 129 avg reward : 1396.3502258103479 policy loss : -125785.578125 KL divergence : 9917.43359375\n",
      "after 139 avg reward : 1505.7794346179805 policy loss : -130229.796875 KL divergence : 9761.771484375\n"
     ]
    }
   ],
   "source": [
    "#primary program\n",
    "gamma = 0.99\n",
    "\n",
    "with tf.device('GPU:0'):\n",
    "    required_reward = 3500  # -150 for pendulum\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "    ploss, kloss, closs1, closs2 = [], [], [], []\n",
    "    ep_ploss, ep_kloss =0,0\n",
    "    kl = 0\n",
    "    pl = 0\n",
    "\n",
    "    num_episodes = 5000\n",
    "    num_steps = 0\n",
    "    target = False\n",
    "    steps = 0\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3E(env, act_dim, act_limit, state_dim)\n",
    "    #load weights saved earlier\n",
    "    #load_saved_weights(agent)\n",
    "    count_s = 0\n",
    "\n",
    "    for eps in range(num_episodes):\n",
    "        if target == True:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        ret = 0\n",
    "        ep_reward = []\n",
    "        done = False\n",
    "        count = 0\n",
    "        ep_ploss = 0\n",
    "        ep_kloss = 0\n",
    " \n",
    "    #for steps in range(num_steps):\n",
    "        while count < 1000:\n",
    "            action, prob, mu, log_std =  agent.policy.act(state[np.newaxis])\n",
    "            ##print(action,prob,l_prob[0])\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            #print(f' s: {state} actins {action} reward {reward} next states : {next_state} done : {done}')\n",
    "            buffer.add(state, action[0], mu[0], log_std[0], reward, next_state, done)\n",
    "\n",
    "            count_s += 1\n",
    "            count += 1\n",
    "            #if count % 5 == 0:\n",
    "            pl, cl1, cl2, kl = agent.train_step(count)\n",
    "            ep_ploss += pl\n",
    "            ep_kloss += kl            \n",
    "                #closs1.append(cl1)\n",
    "                #closs2.append(cl2)\n",
    "              \n",
    "            state = next_state\n",
    "            ret += reward                       \n",
    "            #if done:\n",
    "            #    break\n",
    "        ploss.append(ep_ploss)\n",
    "        kloss.append(ep_kloss) \n",
    "        total_avg_reward.append(ret)\n",
    "        steps += 1\n",
    "        if steps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {eps} avg reward : {avg_rew} policy loss : {np.mean(ploss[-10:])} KL divergence : {np.mean(kloss[-10:])}')\n",
    "        if ret > required_reward: #specific for pendulum, change for different environments e.g. for lunarlander-v0 required reward should be greater than 150\n",
    "            print(f'Episode {eps} return : {ret}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gymv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
