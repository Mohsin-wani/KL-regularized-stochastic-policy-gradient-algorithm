{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pybullet_envs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=1888):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <HalfCheetahBulletEnv<HalfCheetahBulletEnv-v0>>\n",
      "State shape: (26,)\n",
      "Action shape: (6,)\n",
      "action space Box(-1.0, 1.0, (6,), float32) observation space : Box(-inf, inf, (26,), float32)\n",
      "action space bound :[-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.]\n",
      "action limit = 1.0 dimension 6\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = 'HalfCheetahBulletEnv-v0' #  'HopperBulletEnv-v0' # 'AntBulletEnv-v0' \n",
    "seed = 1888\n",
    "env = make_env(env_name,seed )\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_p(tf.keras.Model):\n",
    "    def __init__(self, act_dim, act_limit):\n",
    "        super(Policy_p, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(400, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(300, activation=\"relu\")\n",
    "        self.mean = tf.keras.layers.Dense(act_dim, activation = 'tanh')\n",
    "        self.log_std_dev = tf.keras.layers.Dense(act_dim, activation='linear')\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        mu = self.mean(x)\n",
    "        log_std = self.log_std_dev(x)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "\n",
    "        mu, log_std = self.call(state)\n",
    "        std_dev = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mu,std_dev)\n",
    "        z = normal.sample() \n",
    "        actions = tf.clip_by_value(z,-self.act_limit,self.act_limit)\n",
    "        probs = normal.prob(actions)\n",
    "        \n",
    "        return actions, probs, mu, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, mu, log_std,  reward, next_state, done):\n",
    "        item = (state, action, mu, log_std, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions,  mus, log_stds, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(mus, np.float32), np.array(log_stds, np.float32),  np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, mus,log_stds, rewards, next_states, done_flags = self.sample(32)\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        mus = np.reshape(mus, (-1,act_dim))\n",
    "        log_stds = np.reshape(log_stds, (-1,act_dim))\n",
    "\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        mus_ts = tf.convert_to_tensor(mus, dtype = tf.float32)\n",
    "        log_stds_ts = tf.convert_to_tensor(log_stds, dtype = tf.float32)\n",
    "\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        return state_ts, action_ts, mus_ts, log_stds_ts, reward_ts, next_state_ts, done_flags\n",
    "    \n",
    "    def initialize_replay_buffer(self,env, n_steps = 100):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            buffer.add(state, action,action,action, reward, next_state, done) #action values added as place holders during initialization\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3 with KL regularizer\n",
    "class AgentTD3E:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.dist = tfp.distributions.Normal(0,0.3)\n",
    "        self.learning_rate_critic = 0.002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic()\n",
    "        self.critic2 = Critic()\n",
    "\n",
    "        self.target_critic1 = Critic()\n",
    "        self.target_critic2 = Critic()\n",
    "\n",
    "        self.policy = Policy_p(act_dim,act_limit)\n",
    "        self.target_policy = Policy_p(act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _,_ = self.policy(s)\n",
    "        _,_ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99, eps=1e-6):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions, _, _, _ = self.target_policy.act(next_states)\n",
    "          \n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    #@tf.function\n",
    "    def compute_p_loss(self,states, mean_prev, log_std_prev, eps=1e-6):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions, _, mu_s, log_stds = self.policy.act(states) \n",
    "\n",
    "\n",
    "            kl = (log_stds - log_std_prev) + ( tf.square(tf.exp(log_std_prev)) + ((mu_s - mean_prev) **2) ) / (2 * tf.square(tf.exp(log_stds))) - 0.5 \n",
    "            kl = tf.reduce_mean(tf.reduce_sum(kl, axis = -1))        \n",
    "            policy_loss = - (self.critic1(states,actions) - (0.01 * kl))\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss, kl\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, mus, log_stds, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "\n",
    "        p_loss, kl = self.compute_p_loss(states, mus, log_stds)\n",
    "                \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2, kl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 9 avg reward : -104.37042522614031 policy loss : -21463.99609375 KL divergence : 17648.68359375\n",
      "after 19 avg reward : 510.61375211073727 policy loss : -49655.43359375 KL divergence : 13267.1298828125\n",
      "after 29 avg reward : 461.5111292433927 policy loss : -51301.97265625 KL divergence : 10582.9404296875\n",
      "after 39 avg reward : 575.9824864530946 policy loss : -53027.48828125 KL divergence : 9626.498046875\n",
      "after 49 avg reward : 492.0616242993092 policy loss : -53600.65625 KL divergence : 9523.65234375\n",
      "after 59 avg reward : 607.8737552473008 policy loss : -56259.0390625 KL divergence : 8822.1494140625\n",
      "after 69 avg reward : 679.7994963256522 policy loss : -60313.04296875 KL divergence : 8646.765625\n",
      "after 79 avg reward : 719.2247951244117 policy loss : -63711.6953125 KL divergence : 9975.712890625\n",
      "after 89 avg reward : 817.0406942845774 policy loss : -70100.6953125 KL divergence : 9818.9716796875\n",
      "after 99 avg reward : 802.9294309536656 policy loss : -76224.859375 KL divergence : 9733.9814453125\n",
      "after 109 avg reward : 881.9455004648238 policy loss : -79744.953125 KL divergence : 8735.380859375\n",
      "after 119 avg reward : 859.8308755363423 policy loss : -82676.7265625 KL divergence : 8669.0927734375\n",
      "after 129 avg reward : 933.1483062310529 policy loss : -86945.109375 KL divergence : 7955.2705078125\n",
      "after 139 avg reward : 786.3718805099946 policy loss : -89950.7265625 KL divergence : 8363.236328125\n",
      "after 149 avg reward : 1087.6226146538377 policy loss : -94969.1796875 KL divergence : 9027.544921875\n",
      "after 159 avg reward : 1108.860861713504 policy loss : -98832.0703125 KL divergence : 8832.4423828125\n",
      "after 169 avg reward : 1131.0997022874687 policy loss : -103186.2421875 KL divergence : 9013.0615234375\n",
      "after 179 avg reward : 1178.9292346878108 policy loss : -108017.2109375 KL divergence : 8604.6162109375\n",
      "after 189 avg reward : 1241.220335663767 policy loss : -112576.0859375 KL divergence : 8615.833984375\n",
      "after 199 avg reward : 1288.195471028349 policy loss : -116895.0 KL divergence : 8640.013671875\n",
      "after 209 avg reward : 1324.4330848872764 policy loss : -123079.390625 KL divergence : 9263.14453125\n",
      "after 219 avg reward : 1376.7570979306245 policy loss : -127721.25 KL divergence : 9353.333984375\n",
      "after 229 avg reward : 1392.571960919997 policy loss : -132577.53125 KL divergence : 9168.7763671875\n",
      "after 239 avg reward : 1436.5636947570963 policy loss : -135511.0 KL divergence : 8676.8837890625\n",
      "after 249 avg reward : 1478.213820326289 policy loss : -137862.40625 KL divergence : 8444.412109375\n",
      "after 259 avg reward : 1499.3184252097974 policy loss : -140875.71875 KL divergence : 8252.318359375\n",
      "after 269 avg reward : 1556.245937891193 policy loss : -143923.5 KL divergence : 8178.59619140625\n",
      "after 279 avg reward : 1566.0160702763612 policy loss : -147445.9375 KL divergence : 8083.56787109375\n",
      "after 289 avg reward : 1534.3324734812159 policy loss : -148718.546875 KL divergence : 7924.79052734375\n",
      "after 299 avg reward : 1634.9164771876424 policy loss : -151252.125 KL divergence : 7779.4951171875\n",
      "after 309 avg reward : 1654.6330314643346 policy loss : -154848.15625 KL divergence : 7662.4814453125\n",
      "after 319 avg reward : 1647.7290835014817 policy loss : -157187.953125 KL divergence : 7426.23193359375\n",
      "after 329 avg reward : 1677.4897385755608 policy loss : -160572.703125 KL divergence : 7324.95849609375\n",
      "after 339 avg reward : 1728.7498722886746 policy loss : -162213.015625 KL divergence : 7525.88427734375\n",
      "after 349 avg reward : 1755.9997379455756 policy loss : -165926.40625 KL divergence : 7324.42578125\n",
      "after 359 avg reward : 1819.3219743225036 policy loss : -168873.28125 KL divergence : 7535.92333984375\n",
      "after 369 avg reward : 1810.790438258557 policy loss : -172052.75 KL divergence : 7786.8994140625\n",
      "after 379 avg reward : 1844.9971856812904 policy loss : -174130.015625 KL divergence : 7980.38525390625\n",
      "after 389 avg reward : 1883.3909246761796 policy loss : -176248.5 KL divergence : 8246.826171875\n",
      "after 399 avg reward : 1882.4050898017426 policy loss : -180156.25 KL divergence : 8261.900390625\n",
      "after 409 avg reward : 1850.0784586927657 policy loss : -182525.34375 KL divergence : 8348.052734375\n",
      "after 419 avg reward : 1955.376012810566 policy loss : -184783.34375 KL divergence : 8537.26171875\n",
      "after 429 avg reward : 1941.3039044015525 policy loss : -187823.171875 KL divergence : 8679.6162109375\n",
      "after 439 avg reward : 1973.340246517169 policy loss : -189865.28125 KL divergence : 8733.021484375\n",
      "after 449 avg reward : 1984.9866447049299 policy loss : -193447.21875 KL divergence : 8771.072265625\n",
      "after 459 avg reward : 1996.3883721245525 policy loss : -193953.34375 KL divergence : 8319.220703125\n",
      "after 469 avg reward : 2036.4957546408725 policy loss : -195727.53125 KL divergence : 8397.251953125\n",
      "after 479 avg reward : 2051.1736641028756 policy loss : -196585.21875 KL divergence : 8321.166015625\n",
      "after 489 avg reward : 2042.9552152935307 policy loss : -198171.953125 KL divergence : 8060.890625\n",
      "after 499 avg reward : 2042.6382477675402 policy loss : -199875.15625 KL divergence : 8046.2509765625\n",
      "after 509 avg reward : 2056.8398632381077 policy loss : -201762.09375 KL divergence : 7975.10791015625\n",
      "after 519 avg reward : 2079.623100581459 policy loss : -203370.21875 KL divergence : 7665.421875\n",
      "after 529 avg reward : 2055.2015452027154 policy loss : -203529.9375 KL divergence : 7424.10693359375\n",
      "after 539 avg reward : 2044.6038087837728 policy loss : -204448.265625 KL divergence : 7414.7578125\n",
      "after 549 avg reward : 2087.3450710999027 policy loss : -205399.40625 KL divergence : 7386.4580078125\n",
      "after 559 avg reward : 2049.2664355782417 policy loss : -206851.578125 KL divergence : 7589.8505859375\n",
      "after 569 avg reward : 2087.598444064872 policy loss : -205792.96875 KL divergence : 7668.2919921875\n",
      "after 579 avg reward : 2083.416805821541 policy loss : -205530.546875 KL divergence : 7420.65234375\n",
      "after 589 avg reward : 2077.648177217846 policy loss : -205797.953125 KL divergence : 7498.2041015625\n",
      "after 599 avg reward : 2113.195865032808 policy loss : -205390.046875 KL divergence : 7325.41259765625\n",
      "after 609 avg reward : 2094.349337201076 policy loss : -205933.0 KL divergence : 7077.74853515625\n",
      "after 619 avg reward : 2090.6349272964476 policy loss : -206290.828125 KL divergence : 7094.2529296875\n",
      "after 629 avg reward : 2094.6192640291492 policy loss : -205905.453125 KL divergence : 7040.1982421875\n",
      "after 639 avg reward : 2064.146482940636 policy loss : -207200.0 KL divergence : 7013.00390625\n",
      "after 649 avg reward : 2057.199941800529 policy loss : -206528.28125 KL divergence : 7178.59130859375\n",
      "after 659 avg reward : 2089.443255705585 policy loss : -205453.0625 KL divergence : 7060.55615234375\n",
      "after 669 avg reward : 2090.029936809196 policy loss : -206393.859375 KL divergence : 7147.5439453125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m count_s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     42\u001b[0m count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 44\u001b[0m pl, cl1, cl2, kl \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain_step(count)\n\u001b[0;32m     45\u001b[0m ep_ploss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pl\n\u001b[0;32m     46\u001b[0m ep_kloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m kl            \n",
      "Cell \u001b[1;32mIn[20], line 108\u001b[0m, in \u001b[0;36mAgentTD3E.train_step\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    105\u001b[0m p_loss, kl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_p_loss(states, mus, log_stds)\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolyak_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_critic1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic1)\n\u001b[1;32m--> 108\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolyak_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_critic2, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic2)\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolyak_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_policy, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy)\n\u001b[0;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m p_loss, c_loss1, c_loss2, kl\n",
      "Cell \u001b[1;32mIn[20], line 50\u001b[0m, in \u001b[0;36mAgentTD3E.polyak_update\u001b[1;34m(self, target_network, network)\u001b[0m\n\u001b[0;32m     48\u001b[0m updated_model_weights \u001b[39m=\u001b[39m []\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m weights, target_weights \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(network\u001b[39m.\u001b[39mget_weights(), target_network\u001b[39m.\u001b[39mget_weights()):\n\u001b[1;32m---> 50\u001b[0m     new_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolyak \u001b[39m*\u001b[39;49m target_weights \u001b[39m+\u001b[39;49m ((\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolyak) \u001b[39m*\u001b[39;49m weights)\n\u001b[0;32m     51\u001b[0m     updated_model_weights\u001b[39m.\u001b[39mappend(new_weights)\n\u001b[0;32m     52\u001b[0m target_network\u001b[39m.\u001b[39mset_weights(updated_model_weights)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#primary program\n",
    "gamma = 0.99\n",
    "\n",
    "with tf.device('GPU:0'):\n",
    "    required_reward = 3500\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "    ploss, kloss, closs1, closs2 = [], [], [], []\n",
    "    ep_ploss, ep_kloss =0,0\n",
    "    kl = 0\n",
    "    pl = 0\n",
    "\n",
    "    num_episodes = 5000\n",
    "    num_steps = 0\n",
    "    target = False\n",
    "    steps = 0\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3E(env, act_dim, act_limit, state_dim)\n",
    "    count_s = 0\n",
    "\n",
    "    for eps in range(num_episodes):\n",
    "        if target == True:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        ret = 0\n",
    "        ep_reward = []\n",
    "        done = False\n",
    "        count = 0\n",
    "        ep_ploss = 0\n",
    "        ep_kloss = 0\n",
    " \n",
    "\n",
    "        while count < 1000:\n",
    "            action, prob, mu, log_std =  agent.policy.act(state[np.newaxis]) \n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            buffer.add(state, action[0], mu[0], log_std[0], reward, next_state, done)\n",
    "\n",
    "            count_s += 1\n",
    "            count += 1\n",
    "\n",
    "            pl, cl1, cl2, kl = agent.train_step(count)\n",
    "            ep_ploss += pl\n",
    "            ep_kloss += kl            \n",
    "            state = next_state\n",
    "            ret += reward                       \n",
    "\n",
    "        ploss.append(ep_ploss)\n",
    "        kloss.append(ep_kloss) \n",
    "        total_avg_reward.append(ret)\n",
    "        steps += 1\n",
    "        if steps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {eps} avg reward : {avg_rew} policy loss : {np.mean(ploss[-10:])} KL divergence : {np.mean(kloss[-10:])}')\n",
    "        if ret > required_reward: #specific for pendulum, change for different environments e.g. for lunarlander-v0 required reward should be greater than 150\n",
    "            print(f'Episode {eps} return : {ret}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gymv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
